{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94de836f",
   "metadata": {},
   "source": [
    "# 6章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4abb8a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path(\".\") / \".env\"\n",
    "load_dotenv(dotenv_path=env_path)  # 環境変数を読み込む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cec87a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ドキュメントは別リポジトリで管理されている\n",
    "\n",
    "# from langchain_community.document_loaders import GitLoader\n",
    "\n",
    "\n",
    "# def file_filter(file_path: str) -> bool:\n",
    "#     return file_path.endswith(\".mdx\")A\n",
    "\n",
    "\n",
    "# loader = GitLoader(\n",
    "#     clone_url=\"https://github.com/langchain-ai/langchain\",\n",
    "#     repo_path=\"./langchain\",\n",
    "#     branch=\"master\",\n",
    "#     file_filter=file_filter,\n",
    "# )\n",
    "\n",
    "# documents = loader.load()\n",
    "# print(len(documents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f720dca",
   "metadata": {},
   "source": [
    "### データベース化対象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b70c03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded documents: 1979\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import GitLoader\n",
    "\n",
    "\n",
    "def file_filter(file_path: str) -> bool:\n",
    "    return file_path.endswith(\".mdx\")\n",
    "\n",
    "\n",
    "loader = GitLoader(\n",
    "    clone_url=\"https://github.com/langchain-ai/docs\",\n",
    "    repo_path=\"./docs\",\n",
    "    branch=\"main\",\n",
    "    file_filter=file_filter,\n",
    ")\n",
    "\n",
    "documents = loader.load()\n",
    "print(\"Loaded documents:\", len(documents))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f7584d",
   "metadata": {},
   "source": [
    "### ベクトル化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cf06e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled documents: 494\n",
      "Split into chunks: 1613\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import random\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# ----------------------\n",
    "# 2. サンプリングで件数削減\n",
    "# ----------------------\n",
    "sample_fraction = 0.25\n",
    "sampled_documents = random.sample(documents, k=int(len(documents) * sample_fraction))\n",
    "print(\"Sampled documents:\", len(sampled_documents))\n",
    "\n",
    "# ----------------------\n",
    "# 3. チャンク化\n",
    "# ----------------------\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=4000,  # 1チャンクあたり最大4000文字\n",
    "    chunk_overlap=100,  # 200文字重複で文脈をつなぐ\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(sampled_documents)\n",
    "print(\"Split into chunks:\", len(split_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d395241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# 4. Embeddings 作成 & Chroma登録\n",
    "# ----------------------\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "db = Chroma(embedding_function=embeddings, persist_directory=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "048e7ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab79bba0a42b4bbb8bedbbbb17ee038b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm  # ←Notebook用\n",
    "\n",
    "batch_size = 100\n",
    "for i in tqdm(range(0, len(split_docs), batch_size)):\n",
    "    batch = split_docs[i : i + batch_size]\n",
    "    db.add_documents(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6f0d836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、主に自然言語処理やAIアプリケーションの開発を支援するためのSDK（ソフトウェア開発キット）です。特に、JavaScript向けのモジュールが提供されており、開発者はこれを利用して言語モデルやデータ処理の機能を簡単に統合できます。また、LangChain Academyというリソースもあり、ここではLangChainの使い方や関連技術について学ぶことができます。詳細な情報は、[LangChain SDKの公式ドキュメント](https://reference.langchain.com/javascript/modules/langchain.html)や[LangChain Academy](https://academy.langchain.com/)で確認できます。'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('''\\\n",
    "以下の文脈だけを踏まえて質問に回答してください。\n",
    "\n",
    "文脈: \"\"\"\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "質問: {question}\n",
    "''')\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": retriever,\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke(\"LangChainの概要を教えて\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10231fb",
   "metadata": {},
   "source": [
    "## 6.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd76bc4",
   "metadata": {},
   "source": [
    "### HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09642983",
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothetical_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "次の質問に回答する一文を書いてください。\n",
    "\n",
    "質問: {question}\n",
    "\"\"\")\n",
    "\n",
    "hypothetical_chain = hypothetical_prompt | model | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16498213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、言語モデル（LLM）を使用してアプリケーションを構築するためのフレームワークです。主に以下の特徴があります：\\n\\n1. **簡単なスタート**: LangChainは、LLMを使ったアプリケーションの構築を容易にすることを目的としています。開発者がすぐに始められるように設計されています。\\n\\n2. **柔軟性と生産性**: LangChainは、さまざまなモデルプロバイダーやデータソースと統合できる柔軟性を持ち、プロダクション環境でも使用できるように設計されています。\\n\\n3. **コンポーネントのチェーン化**: LangChainは、プロンプトテンプレート、モデル、出力パーサーなどの基本的なコンポーネントを使用して、複雑なフローを構築することができます。これにより、開発者は異なるコンポーネントを組み合わせて、より高度なアプリケーションを作成できます。\\n\\n4. **エージェント的なアプリケーション**: LangChainは、将来的なアプリケーションがよりエージェント的になると考えており、LLMを使ってデータや計算と相互作用する複雑なフローをオーケストレーションすることを目指しています。\\n\\n5. **トレーシングとデプロイ**: LangSmithを使用することで、アプリケーションのトレーシングやデバッグが可能になり、LangServeを使ってアプリケーションをREST APIとしてデプロイすることもできます。\\n\\nLangChainは、開発者が最新のモデルを簡単に利用できるようにし、アプリケーションの構築を効率化することを目指しています。'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyde_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": hypothetical_chain | retriever,\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "hyde_rag_chain.invoke(\"LangChainの概要を教えて\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51630fc0",
   "metadata": {},
   "source": [
    "### 複数の検索クエリの生成\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3dd8f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'properties': {'queries': {'description': '検索クエリのリスト',\n",
      "                            'items': {'type': 'string'},\n",
      "                            'title': 'Queries',\n",
      "                            'type': 'array'}},\n",
      " 'required': ['queries'],\n",
      " 'title': 'QueryGenerationOutput',\n",
      " 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class QueryGenerationOutput(BaseModel):\n",
    "    queries: list[str] = Field(..., description=\"検索クエリのリスト\")\n",
    "\n",
    "\n",
    "pprint.pprint(QueryGenerationOutput.model_json_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "307da07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatPromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='質問に対してベクターデータベースから関連文書を検索するために、\\n3つの異なる検索クエリを生成してください。\\n距離ベースの類似性検索の限界を克服するために、\\nユーザーの質問に対して複数の視点を提供することが目標です。\\n\\n質問: {question}\\n'), additional_kwargs={})])\n",
      "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x70820ac21940>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x70820ac23380>, root_client=<openai.OpenAI object at 0x70820ade6850>, root_async_client=<openai.AsyncOpenAI object at 0x70820ac34190>, model_name='gpt-4o-mini', temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True), kwargs={'response_format': <class '__main__.QueryGenerationOutput'>, 'ls_structured_output_format': {'kwargs': {'method': 'json_schema', 'strict': None}, 'schema': {'type': 'function', 'function': {'name': 'QueryGenerationOutput', 'description': '', 'parameters': {'properties': {'queries': {'description': '検索クエリのリスト', 'items': {'type': 'string'}, 'type': 'array'}}, 'required': ['queries'], 'type': 'object'}}}}}, config={}, config_factories=[])\n",
      "| RunnableBinding(bound=RunnableLambda(...), kwargs={}, config={}, config_factories=[], custom_output_type=<class '__main__.QueryGenerationOutput'>)\n",
      "| RunnableLambda(...)\n"
     ]
    }
   ],
   "source": [
    "query_generation_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "質問に対してベクターデータベースから関連文書を検索するために、\n",
    "3つの異なる検索クエリを生成してください。\n",
    "距離ベースの類似性検索の限界を克服するために、\n",
    "ユーザーの質問に対して複数の視点を提供することが目標です。\n",
    "\n",
    "質問: {question}\n",
    "\"\"\")\n",
    "\n",
    "query_generation_chain = (\n",
    "    query_generation_prompt\n",
    "    | model.with_structured_output(QueryGenerationOutput)\n",
    "    | (lambda x: x.queries)\n",
    ")\n",
    "\n",
    "pprint.pprint(query_generation_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c455f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChainは、言語モデル（LLM）を活用したアプリケーションを構築するためのフレームワークです。主な特徴は以下の通りです：\n",
      "\n",
      "1. **モジュール性**: LangChainは、さまざまなモジュールを提供しており、これらを単独で使用したり、複雑なユースケースのために組み合わせたりすることができます。\n",
      "\n",
      "2. **コンポーネントのチェーン**: LangChainは、コンポーネントをシームレスに連結するためのLangChain Expression Language（LCEL）を使用しており、これにより複雑なフローを簡単に構築できます。\n",
      "\n",
      "3. **基本的なコンポーネント**: LangChainのアプリケーションは、主に以下の3つのコンポーネントで構成されます：\n",
      "   - **LLM/チャットモデル**: 言語モデルのコアエンジン。\n",
      "   - **プロンプトテンプレート**: 言語モデルに指示を与えるためのもの。\n",
      "   - **出力パーサー**: 言語モデルからの生の応答をより扱いやすい形式に変換します。\n",
      "\n",
      "4. **開発とデプロイ**: LangChainは、アプリケーションの開発、テスト、デプロイをサポートするためのツール（LangSmithやLangServeなど）を提供しています。LangSmithはアプリケーションのトレースやデバッグを行うためのプラットフォームであり、LangServeはLangChainのチェーンをREST APIとしてデプロイするためのライブラリです。\n",
      "\n",
      "5. **柔軟性と拡張性**: LangChainは、さまざまなモデルプロバイダーやデータストア、APIとの統合を容易にし、開発者が最新のモデルを簡単に利用できるように設計されています。\n",
      "\n",
      "LangChainは、言語モデルを利用したアプリケーションの開発を簡素化し、将来的なアプリケーションの可能性を広げることを目指しています。\n"
     ]
    }
   ],
   "source": [
    "multi_query_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": query_generation_chain | retriever.map(),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "out = multi_query_rag_chain.invoke(\"LangChainの概要を教えて\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa705d6",
   "metadata": {},
   "source": [
    "## 6.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d676a9",
   "metadata": {},
   "source": [
    "### RAG Fusion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf7c6e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(\n",
    "    retriever_outputs: list[list[Document]],\n",
    "    k: int = 60,\n",
    ") -> list[str]:\n",
    "    # 各ドキュメントのコンテンツ (文字列) とそのスコアの対応を保持する辞書を準備\n",
    "    content_score_mapping = {}\n",
    "\n",
    "    # 検索クエリごとにループ\n",
    "    for docs in retriever_outputs:\n",
    "        # 検索結果のドキュメントごとにループ\n",
    "        for rank, doc in enumerate(docs):\n",
    "            content = doc.page_content\n",
    "\n",
    "            # 初めて登場したコンテンツの場合はスコアを0で初期化\n",
    "            if content not in content_score_mapping:\n",
    "                content_score_mapping[content] = 0\n",
    "\n",
    "            # (1 / (順位 + k)) のスコアを加算\n",
    "            content_score_mapping[content] += 1 / (rank + k)\n",
    "\n",
    "    # スコアの大きい順にソート\n",
    "    ranked = sorted(content_score_mapping.items(), key=lambda x: x[1], reverse=True)  # noqa\n",
    "    return [content for content, _ in ranked]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27794293",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、言語モデル（LLM）を使用してアプリケーションを構築するためのフレームワークです。主な目的は、開発者がLLMを簡単に利用できるようにし、柔軟で生産性の高いアプリケーションを作成できるようにすることです。LangChainは、以下のような特徴を持っています。\\n\\n1. **モジュールの統合**: LangChainは、プロンプトテンプレート、モデル、出力パーサーなどの基本的なコンポーネントを提供し、これらを組み合わせて複雑なフローを構築できます。\\n\\n2. **エコシステムのサポート**: LangChainは、OpenAIなどのモデルプロバイダーやデータストア、APIとの統合を必要とし、これにより外部データソースと連携したアプリケーションの開発が可能です。\\n\\n3. **トレーシングとデバッグ**: LangSmithというツールを使用することで、アプリケーションのトレーシングやデバッグが容易になり、複雑なアプリケーションの内部を可視化できます。\\n\\n4. **デプロイメント**: LangServeを使用することで、LangChainのチェーンをREST APIとしてデプロイすることができます。\\n\\n5. **進化する技術**: LangChainは、LLMの進化に合わせて常に更新されており、開発者が最新の技術を利用できるようにしています。\\n\\nLangChainは、開発者がLLMを活用して、よりインテリジェントでエージェント的なアプリケーションを構築するための基盤を提供します。'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_fusion_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": query_generation_chain | retriever.map() | reciprocal_rank_fusion,\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_fusion_chain.invoke(\"LangChainの概要を教えて\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5d8a51",
   "metadata": {},
   "source": [
    "### Cohere のリランクモデルの導入\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7cd5744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# バージョン合わなかったので略"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4fe071",
   "metadata": {},
   "source": [
    "## 6.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc3d8b1",
   "metadata": {},
   "source": [
    "### LLM によるルーティング\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86865647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import TavilySearchAPIRetriever\n",
    "\n",
    "\n",
    "langchain_document_retriever = retriever.with_config(\n",
    "    {\"run_name\": \"langchain_document_retriever\"}\n",
    ")\n",
    "\n",
    "web_retriever = TavilySearchAPIRetriever(k=3).with_config({\"run_name\": \"web_retriever\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50f57f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class Route(str, Enum):\n",
    "    langchain_document = \"langchain_document\"\n",
    "    web = \"web\"\n",
    "\n",
    "\n",
    "class RouteOutput(BaseModel):\n",
    "    route: Route\n",
    "\n",
    "\n",
    "route_prompt = ChatPromptTemplate.from_template(\"\"\"\\\n",
    "質問に回答するために適切なRetrieverを選択してください。\n",
    "\n",
    "質問: {question}\n",
    "\"\"\")\n",
    "\n",
    "route_chain = (\n",
    "    route_prompt | model.with_structured_output(RouteOutput) | (lambda x: x.route)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd2e0e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "\n",
    "def routed_retriever(inp: dict[str, Any]) -> list[Document]:\n",
    "    question = inp[\"question\"]\n",
    "    route = inp[\"route\"]\n",
    "\n",
    "    if route == Route.langchain_document:\n",
    "        return langchain_document_retriever.invoke(question)\n",
    "    elif route == Route.web:\n",
    "        return web_retriever.invoke(question)\n",
    "\n",
    "    raise ValueError(f\"Unknown route: {route}\")\n",
    "\n",
    "\n",
    "route_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"route\": route_chain,\n",
    "    }\n",
    "    | RunnablePassthrough.assign(context=routed_retriever)\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1e46db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、主に自然言語処理やAIアプリケーションの開発を支援するためのSDK（ソフトウェア開発キット）です。LangChainを使用することで、開発者は言語モデルを活用したアプリケーションを簡単に構築できるようになります。公式のリファレンスや学習リソースは、LangChainの機能や使い方を学ぶために提供されています。具体的には、LangChainのリファレンスは[こちら](https://reference.langchain.com/javascript/modules/langchain.html)で、学習リソースは[こちら](https://academy.langchain.com/)からアクセスできます。'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "route_rag_chain.invoke(\"LangChainの概要を教えて\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7013025c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'東京の今日の天気は、午前中に雨が降る可能性があり、その後は日差しが出るチャンスもありますが、夕方以降は再び雨の可能性があります。外出する際には雨具を持っていくことをおすすめします。また、全体的に曇りがちで、気温は低めになる見込みです。'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "route_rag_chain.invoke(\"東京の今日の天気は？\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e99b14c",
   "metadata": {},
   "source": [
    "### ハイブリッド検索の実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64304e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "chroma_retriever = retriever.with_config({\"run_name\": \"chroma_retriever\"})\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(documents).with_config(\n",
    "    {\"run_name\": \"bm25_retriever\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7cebe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "hybrid_retriever = (\n",
    "    RunnableParallel(\n",
    "        {\n",
    "            \"chroma_documents\": chroma_retriever,\n",
    "            \"bm25_documents\": bm25_retriever,\n",
    "        }\n",
    "    )\n",
    "    | (lambda x: [x[\"chroma_documents\"], x[\"bm25_documents\"]])\n",
    "    | reciprocal_rank_fusion  # 最後にソート\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b261d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChainは、AIアプリケーションを構築するためのフレームワークであり、特に大規模言語モデル（LLM）を活用したアプリケーションやエージェントの開発を支援します。LangChainを使用することで、開発者は複数のAPI呼び出しを組み合わせてユーザーのリクエストに応じた応答を生成することができます。\\n\\nLangChainは、以下のような機能を提供します：\\n\\n1. **エージェントの構築**: ユーザーのリクエストに基づいて、複数のツールやAPIを組み合わせて処理を行うエージェントを作成できます。\\n2. **ログとトレース**: Portkeyとの統合により、すべてのリクエストをログに記録し、トレースIDを使用してリクエストの可視化を行うことができます。これにより、ユーザーのインタラクションを詳細に分析できます。\\n3. **キャッシングとリトライ**: 過去のリクエストをキャッシュして再利用することで、コストを削減し、応答時間を短縮することができます。また、失敗したリクエストを自動的に再処理する機能も提供されています。\\n4. **セマンティックな制御**: Pebbloとの統合により、データの読み込みと取得において、セマンティックなトピックやエンティティに基づく制御を行うことができます。\\n\\nLangChainは、さまざまなAIプロバイダーやモデルに接続できるため、開発者は柔軟にアプリケーションを構築し、運用することが可能です。'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_rag_chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": hybrid_retriever,\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "hybrid_rag_chain.invoke(\"LangChainの概要を教えて\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd697a77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorial-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
